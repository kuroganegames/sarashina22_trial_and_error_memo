{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c42d332d0d2424a8ab7cac83ef4bad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/6.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a91039c38e34c9896d41a902584a449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4ee16f37c94de794d632718693ff7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sbintuitions/sarashina2.2-0.5b-instruct-v0.1\",\n",
    "    cache_dir=r\"/media/kurogane/HD-NRLD-A/cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{%- set user_messages = messages | selectattr('role', 'equalto', 'user') | list %}\n",
      "{%- macro output_available_tools(tools, message) %}\n",
      "{%- if tools and (message == user_messages[-1]) %}\n",
      "    {{- '<|available_tools|>[' }}\n",
      "    {%- for tool in tools %}\n",
      "        {%- set tool = tool.function %}\n",
      "        {{- \"{\" }}\n",
      "        {%- for key, val in tool.items() if key != \"return\" %}\n",
      "            {%- if val is string %}\n",
      "                {{- \"'\" + key + \"': '\" + val + \"'\" }}\n",
      "            {%- else %}\n",
      "                {{- \"'\" + key + \"': \" + val|string }}\n",
      "            {%- endif %}\n",
      "            {%- if not loop.last %}\n",
      "                {{- \", \" }}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "        {{- \"}\" }}\n",
      "        {%- if not loop.last %}\n",
      "            {{- \", \" }}\n",
      "        {%- else %}\n",
      "            {{- \"]\" }}\n",
      "        {%- endif %}\n",
      "    {%- endfor %}\n",
      "    {{- eos_token -}}\n",
      "{%- endif %}\n",
      "{%- endmacro %}\n",
      "\n",
      "{%- macro output_tool_results(tool_results) %}\n",
      "{{- '<|tool_results|>[' }}\n",
      "{%- for tool_result in tool_results %}\n",
      "    {{- \"{'content': \" + tool_result.content|string + \", 'call_id': '\" + tool_result.call_id + \"'}\" }}\n",
      "{%- endfor %}\n",
      "{{- ']' }}\n",
      "{{- eos_token -}}\n",
      "{%- endmacro %}\n",
      "\n",
      "{%- macro output_tool_calls(tool_calls) %}\n",
      "{{- '<|tool_calls|>[' }}\n",
      "{%- for tool_call in tool_calls %}\n",
      "    {{- \"{'id': '\" + tool_call.id + \"', 'name': '\" + tool_call.name + \"', 'arguments': \" + tool_call.arguments|string + '}' }}\n",
      "{%- endfor %}\n",
      "{{- ']' }}\n",
      "{%- endmacro %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'user' %}\n",
      "        {%- if tools is defined %}\n",
      "            {{- output_available_tools(tools, message) }}\n",
      "        {%- endif %}\n",
      "        {{- '<|user|>' + message['content'] + eos_token -}}\n",
      "    {%- elif message['role'] == 'system' %}\n",
      "        {{- '<|system|>' + message['content'] + eos_token -}}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "        {% set assistant_content = \"\" %}\n",
      "        {%- if message.content is defined %}\n",
      "            {% set assistant_content = message.content %}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls is defined and message.tool_calls -%}\n",
      "            {{- '<|assistant|>' + assistant_content + output_tool_calls(message['tool_calls']) + eos_token -}}\n",
      "        {%- else %}\n",
      "            {{- '<|assistant|>' + assistant_content + eos_token }}\n",
      "        {%- endif %}\n",
      "    {%- elif message['role'] == 'tool_results' %}\n",
      "        {{- output_tool_results(message.tool_results) }}\n",
      "    {%- endif %}\n",
      "{%- if loop.last and add_generation_prompt -%}\n",
      "  {{- '<|assistant|>' -}}\n",
      "{%- endif -%}\n",
      "{%- endfor %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n{%- set user_messages = messages | selectattr('role', 'equalto', 'user') | list %}\n",
      "{%- macro output_available_tools(tools, message) %}\n",
      "{%- if tools and (message == user_messages[-1]) %}\n",
      "    {{- '<|available_tools|>[' }}\n",
      "    {%- for tool in tools %}\n",
      "        {%- set tool = tool.function %}\n",
      "        {{- \"{\" }}\n",
      "        {%- for key, val in tool.items() if key != \"return\" %}\n",
      "            {%- if val is string %}\n",
      "                {{- \"'\" + key + \"': '\" + val + \"'\" }}\n",
      "            {%- else %}\n",
      "                {{- \"'\" + key + \"': \" + val|string }}\n",
      "            {%- endif %}\n",
      "            {%- if not loop.last %}\n",
      "                {{- \", \" }}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "        {{- \"}\" }}\n",
      "        {%- if not loop.last %}\n",
      "            {{- \", \" }}\n",
      "        {%- else %}\n",
      "            {{- \"]\" }}\n",
      "        {%- endif %}\n",
      "    {%- endfor %}\n",
      "    {{- eos_token -}}\n",
      "{%- endif %}\n",
      "{%- endmacro %}\n",
      "\\n{%- macro output_tool_results(tool_results) %}\n",
      "{{- '<|tool_results|>[' }}\n",
      "{%- for tool_result in tool_results %}\n",
      "    {{- \"{'content': \" + tool_result.content|string + \", 'call_id': '\" + tool_result.call_id + \"'}\" }}\n",
      "{%- endfor %}\n",
      "{{- ']' }}\n",
      "{{- eos_token -}}\n",
      "{%- endmacro %}\n",
      "\\n{%- macro output_tool_calls(tool_calls) %}\n",
      "{{- '<|tool_calls|>[' }}\n",
      "{%- for tool_call in tool_calls %}\n",
      "    {{- \"{'id': '\" + tool_call.id + \"', 'name': '\" + tool_call.name + \"', 'arguments': \" + tool_call.arguments|string + '}' }}\n",
      "{%- endfor %}\n",
      "{{- ']' }}\n",
      "{%- endmacro %}\n",
      "\\n{%- for message in messages %}\n",
      "    {%- if message['role'] == 'user' %}\n",
      "        {%- if tools is defined %}\n",
      "            {{- output_available_tools(tools, message) }}\n",
      "        {%- endif %}\n",
      "        {{- '<|user|>' + message['content'] + eos_token -}}\n",
      "    {%- elif message['role'] == 'system' %}\n",
      "        {{- '<|system|>' + message['content'] + eos_token -}}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "        {% set assistant_content = \"\" %}\n",
      "        {%- if message.content is defined %}\n",
      "            {% set assistant_content = message.content %}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls is defined and message.tool_calls -%}\n",
      "            {{- '<|assistant|>' + assistant_content + output_tool_calls(message['tool_calls']) + eos_token -}}\n",
      "        {%- else %}\n",
      "            {{- '<|assistant|>' + assistant_content + eos_token }}\n",
      "        {%- endif %}\n",
      "    {%- elif message['role'] == 'tool_results' %}\n",
      "        {{- output_tool_results(message.tool_results) }}\n",
      "    {%- endif %}\n",
      "{%- if loop.last and add_generation_prompt -%}\n",
      "  {{- '<|assistant|>' -}}\n",
      "{%- endif -%}\n",
      "{%- endfor %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template.replace(\"}\\n\", \"####\").replace(\"\\n\", \"\\\\n\").replace(\"####\", \"}\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_bos_token',\n",
       " '_add_eos_token',\n",
       " '_add_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_call_one',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_pad',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_set_model_specific_special_tokens',\n",
       " '_set_processor_class',\n",
       " '_special_tokens_map',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_upload_modified_files',\n",
       " 'add_bos_token',\n",
       " 'add_eos_token',\n",
       " 'add_prefix_space',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'extra_special_tokens',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'legacy',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'update_post_processor',\n",
       " 'use_default_system_prompt',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_file',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='sbintuitions/sarashina2.2-0.5b-instruct-v0.1', vocab_size=102400, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<|system|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t8: AddedToken(\"<|assistant|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t9: AddedToken(\"<|user|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t10: AddedToken(\"<|available_tools|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t11: AddedToken(\"<|tool_calls|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t12: AddedToken(\"<|tool_results|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t13: AddedToken(\"<|code|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t14: AddedToken(\"<|file|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102397: AddedToken(\"<|prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102398: AddedToken(\"<|suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102399: AddedToken(\"<|middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllmtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
